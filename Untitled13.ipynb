{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPCsIHZHTB+KinVu3NLuqvX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bawcode/ethiopian-coffee-trade-analysis/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2FirCimRqXC",
        "outputId": "a5bb8e49-51a1-408a-9d56-64ca374e734b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
            "Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.9.11\n"
          ]
        }
      ],
      "source": [
        "!pip install psycopg2-binary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Download and Extract the Dataset\n",
        "We’ll use the NYC Yellow Taxi Trip Data (January 2019), which is available as a CSV file (~7 million rows). We’ll download it directly in Colab and sample exactly 2 million rows."
      ],
      "metadata": {
        "id": "vNpMov0uUDpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Download the NYC Taxi Data (January 2019)\n",
        "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet'\n",
        "df = pd.read_parquet(url)\n",
        "# Sample exactly 2 million rows\n",
        "df = df.sample(n=2000000, random_state=42)\n",
        "print(f\"Dataset size: {len(df)} rows\")\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5BAk7HHTSa6",
        "outputId": "06120c6b-7a88-415e-b9a7-64aa214b5c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 2000000 rows\n",
            "         VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
            "1996239         1  2019-01-09 22:54:47   2019-01-09 23:23:09              1.0   \n",
            "2535676         2  2019-01-11 21:30:16   2019-01-11 21:53:14              2.0   \n",
            "1257411         1  2019-01-06 19:06:07   2019-01-06 19:23:40              1.0   \n",
            "2612269         4  2019-01-12 04:23:27   2019-01-12 04:39:41              1.0   \n",
            "7470804         2  2019-01-31 11:46:57   2019-01-31 11:55:58              1.0   \n",
            "\n",
            "         trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n",
            "1996239           6.20         1.0                  N           249   \n",
            "2535676           4.55         1.0                  N           114   \n",
            "1257411           2.60         1.0                  N           163   \n",
            "2612269           4.37         1.0                  N           234   \n",
            "7470804           0.89         1.0                  N            90   \n",
            "\n",
            "         DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
            "1996239           179             1         23.0    0.5      0.5        2.00   \n",
            "2535676           142             2         19.5    0.5      0.5        0.00   \n",
            "1257411           249             1         13.0    0.0      0.5        2.50   \n",
            "2612269           256             1         16.0    0.5      0.5        3.46   \n",
            "7470804           246             1          7.5    0.0      0.5        1.66   \n",
            "\n",
            "         tolls_amount  improvement_surcharge  total_amount  \\\n",
            "1996239           0.0                    0.3         26.30   \n",
            "2535676           0.0                    0.3         20.80   \n",
            "1257411           0.0                    0.3         16.30   \n",
            "2612269           0.0                    0.3         20.76   \n",
            "7470804           0.0                    0.3          9.96   \n",
            "\n",
            "         congestion_surcharge airport_fee  \n",
            "1996239                   NaN        None  \n",
            "2535676                   NaN        None  \n",
            "1257411                   NaN        None  \n",
            "2612269                   NaN        None  \n",
            "7470804                   0.0        None  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2000000 entries, 1996239 to 6643440\n",
            "Data columns (total 19 columns):\n",
            " #   Column                 Dtype         \n",
            "---  ------                 -----         \n",
            " 0   VendorID               int64         \n",
            " 1   tpep_pickup_datetime   datetime64[us]\n",
            " 2   tpep_dropoff_datetime  datetime64[us]\n",
            " 3   passenger_count        float64       \n",
            " 4   trip_distance          float64       \n",
            " 5   RatecodeID             float64       \n",
            " 6   store_and_fwd_flag     object        \n",
            " 7   PULocationID           int64         \n",
            " 8   DOLocationID           int64         \n",
            " 9   payment_type           int64         \n",
            " 10  fare_amount            float64       \n",
            " 11  extra                  float64       \n",
            " 12  mta_tax                float64       \n",
            " 13  tip_amount             float64       \n",
            " 14  tolls_amount           float64       \n",
            " 15  improvement_surcharge  float64       \n",
            " 16  total_amount           float64       \n",
            " 17  congestion_surcharge   float64       \n",
            " 18  airport_fee            object        \n",
            "dtypes: datetime64[us](2), float64(11), int64(4), object(2)\n",
            "memory usage: 305.2+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LEEnNOhKJ2qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDq5WMNmUW3M",
        "outputId": "f841fd5a-2c30-499e-92c6-165360dd52b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000000, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Transform the Data\n",
        "We’ll clean and transform the 2 million rows to prepare them for analysis, ensuring the dataset remains at exactly 2 million rows after cleaning. Transformations will support the business scenario (e.g., analyzing trip patterns).\n",
        "Transformations:\n",
        "\n",
        "Remove Invalid Data: Filter out rows with negative fares, zero distances, or missing critical fields.\n",
        "Convert Data Types: Ensure pickup/dropoff times are datetimes.\n",
        "Feature Engineering: Add columns like trip duration, pickup hour, and day of the week.\n",
        "Maintain 2 Million Rows: If cleaning reduces the row count, reload additional rows to compensate."
      ],
      "metadata": {
        "id": "noVzIS9UT8sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert datetime columns\n",
        "df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
        "df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
        "\n",
        "# Filter invalid data\n",
        "df_clean = df[\n",
        "    (df['fare_amount'] > 0) &\n",
        "    (df['trip_distance'] > 0) &\n",
        "    (df['passenger_count'] > 0) &\n",
        "    (df['tpep_pickup_datetime'].notnull()) &\n",
        "    (df['tpep_dropoff_datetime'].notnull())\n",
        "]\n",
        "\n",
        "# Calculate trip duration (in minutes)\n",
        "df_clean['trip_duration'] = (df_clean['tpep_dropoff_datetime'] - df_clean['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
        "\n",
        "# Extract hour and day of week\n",
        "df_clean['pickup_hour'] = df_clean['tpep_pickup_datetime'].dt.hour\n",
        "df_clean['pickup_day'] = df_clean['tpep_pickup_datetime'].dt.day_name()\n",
        "\n",
        "# Select relevant columns\n",
        "df_clean = df_clean[[\n",
        "    'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count',\n",
        "    'trip_distance', 'fare_amount', 'PULocationID', 'DOLocationID',\n",
        "    'trip_duration', 'pickup_hour', 'pickup_day'\n",
        "]]\n",
        "\n",
        "# Check row count and adjust if needed\n",
        "print(f\"Rows after cleaning: {len(df_clean)}\")\n",
        "if len(df_clean) > 2000000:\n",
        "    df_clean = df_clean.sample(n=2000000, random_state=42)\n",
        "elif len(df_clean) < 2000000:\n",
        "    # Load additional rows to compensate for filtered data\n",
        "    additional_rows_needed = 2000000 - len(df_clean)\n",
        "    df_additional = pd.read_parquet(url).sample(n=additional_rows_needed + 10000, random_state=43)\n",
        "    df_additional['tpep_pickup_datetime'] = pd.to_datetime(df_additional['tpep_pickup_datetime'])\n",
        "    df_additional['tpep_dropoff_datetime'] = pd.to_datetime(df_additional['tpep_dropoff_datetime'])\n",
        "    df_additional = df_additional[\n",
        "        (df_additional['fare_amount'] > 0) &\n",
        "        (df_additional['trip_distance'] > 0) &\n",
        "        (df_additional['passenger_count'] > 0) &\n",
        "        (df_additional['tpep_pickup_datetime'].notnull()) &\n",
        "        (df_additional['tpep_dropoff_datetime'].notnull())\n",
        "    ]\n",
        "    df_additional['trip_duration'] = (df_additional['tpep_dropoff_datetime'] - df_additional['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
        "    df_additional['pickup_hour'] = df_additional['tpep_pickup_datetime'].dt.hour\n",
        "    df_additional['pickup_day'] = df_additional['tpep_pickup_datetime'].dt.day_name()\n",
        "    df_additional = df_additional[df_clean.columns]\n",
        "    df_clean = pd.concat([df_clean, df_additional]).sample(n=2000000, random_state=42)\n",
        "\n",
        "print(f\"Final dataset size: {len(df_clean)} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7d7RzrMTY-j",
        "outputId": "7f2f8083-6e73-4896-e34e-d4d6b07428bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-70555697.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_clean['trip_duration'] = (df_clean['tpep_dropoff_datetime'] - df_clean['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
            "/tmp/ipython-input-70555697.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_clean['pickup_hour'] = df_clean['tpep_pickup_datetime'].dt.hour\n",
            "/tmp/ipython-input-70555697.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_clean['pickup_day'] = df_clean['tpep_pickup_datetime'].dt.day_name()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows after cleaning: 1946591\n",
            "Final dataset size: 2000000 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%sql \\dt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4NCk1vPazLU",
        "outputId": "1c135ab4-fe7f-4647-84b9-e34307dac43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%sql` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "\n",
        "CREATE TABLE taxi_trips (\n",
        "    trip_id SERIAL PRIMARY KEY,\n",
        "    pickup_datetime TIMESTAMP NOT NULL,\n",
        "    dropoff_datetime TIMESTAMP NOT NULL,\n",
        "    passenger_count INTEGER NOT NULL,\n",
        "    trip_distance FLOAT NOT NULL,\n",
        "    fare_amount FLOAT NOT NULL,\n",
        "    pickup_location_id INTEGER,\n",
        "    dropoff_location_id INTEGER,\n",
        "    trip_duration FLOAT,\n",
        "    pickup_hour INTEGER,\n",
        "    pickup_day VARCHAR(10)\n",
        ");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "MTReThryTZkT",
        "outputId": "390515ee-c0ea-434d-952a-10461331a16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1164064296.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1164064296.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    CREATE TABLE taxi_trips (\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oJc7ScdWaf30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up database connection\n",
        "db_url = 'postgresql+psycopg2://username:password@host:port/dbname'  # Replace with ElephantSQL details\n",
        "engine = create_engine(db_url)\n",
        "\n",
        "# Load data in chunks\n",
        "chunk_size = 100000\n",
        "for i in range(0, len(df_clean), chunk_size):\n",
        "    chunk = df_clean[i:i + chunk_size]\n",
        "    chunk.to_sql('taxi_trips', engine, if_exists='append', index=False)\n",
        "    print(f\"Loaded {i + len(chunk)} rows\")\n",
        "\n",
        "print(\"Data loading complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "rpxfq5W0TeQ7",
        "outputId": "f359c487-f3d9-4406-eb75-423473d87590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'port'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-557752335.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set up database connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdb_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'postgresql+psycopg2://username:password@host:port/dbname'\u001b[0m  \u001b[0;31m# Replace with ElephantSQL details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load data in chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/util/deprecations.py\u001b[0m in \u001b[0;36mwarned\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m                         \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                     )\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/create.py\u001b[0m in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;31m# create url.URL object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instantiate_plugins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/url.py\u001b[0m in \u001b[0;36mmake_url\u001b[0;34m(name_or_url)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_parse_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m     elif not isinstance(name_or_url, URL) and not hasattr(\n\u001b[1;32m    858\u001b[0m         \u001b[0mname_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_sqla_is_testing_if_this_is_a_mock_object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sqlalchemy/engine/url.py\u001b[0m in \u001b[0;36m_parse_url\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"port\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"port\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"port\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mURL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'port'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the database\n",
        "query = \"\"\"\n",
        "SELECT pickup_hour, AVG(fare_amount) as avg_fare, AVG(trip_duration) as avg_duration\n",
        "FROM taxi_trips\n",
        "GROUP BY pickup_hour\n",
        "ORDER BY pickup_hour;\n",
        "\"\"\"\n",
        "result = pd.read_sql(query, engine)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "nKwdxlczThAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install psycopg2-binary\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Step 1: Download and extract data\n",
        "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet'\n",
        "df = pd.read_parquet(url)\n",
        "df = df.sample(n=2000000, random_state=42)\n",
        "print(f\"Dataset size: {len(df)} rows\")\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "\n",
        "# Step 2: Transform data\n",
        "df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
        "df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
        "df_clean = df[\n",
        "    (df['fare_amount'] > 0) &\n",
        "    (df['trip_distance'] > 0) &\n",
        "    (df['passenger_count'] > 0) &\n",
        "    (df['tpep_pickup_datetime'].notnull()) &\n",
        "    (df['tpep_dropoff_datetime'].notnull())\n",
        "]\n",
        "df_clean['trip_duration'] = (df_clean['tpep_dropoff_datetime'] - df_clean['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
        "df_clean['pickup_hour'] = df_clean['tpep_pickup_datetime'].dt.hour\n",
        "df_clean['pickup_day'] = df_clean['tpep_pickup_datetime'].dt.day_name()\n",
        "df_clean = df_clean[[\n",
        "    'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count',\n",
        "    'trip_distance', 'fare_amount', 'PULocationID', 'DOLocationID',\n",
        "    'trip_duration', 'pickup_hour', 'pickup_day'\n",
        "]]\n",
        "print(f\"Rows after cleaning: {len(df_clean)}\")\n",
        "if len(df_clean) > 2000000:\n",
        "    df_clean = df_clean.sample(n=2000000, random_state=42)\n",
        "elif len(df_clean) < 2000000:\n",
        "    additional_rows_needed = 2000000 - len(df_clean)\n",
        "    df_additional = pd.read_parquet(url).sample(n=additional_rows_needed + 10000, random_state=43)\n",
        "    df_additional['tpep_pickup_datetime'] = pd.to_datetime(df_additional['tpep_pickup_datetime'])\n",
        "    df_additional['tpep_dropoff_datetime'] = pd.to_datetime(df_additional['tpep_dropoff_datetime'])\n",
        "    df_additional = df_additional[\n",
        "        (df_additional['fare_amount'] > 0) &\n",
        "        (df_additional['trip_distance'] > 0) &\n",
        "        (df_additional['passenger_count'] > 0) &\n",
        "        (df_additional['tpep_pickup_datetime'].notnull()) &\n",
        "        (df_additional['tpep_dropoff_datetime'].notnull())\n",
        "    ]\n",
        "    df_additional['trip_duration'] = (df_additional['tpep_dropoff_datetime'] - df_additional['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
        "    df_additional['pickup_hour'] = df_additional['tpep_pickup_datetime'].dt.hour\n",
        "    df_additional['pickup_day'] = df_additional['tpep_pickup_datetime'].dt.day_name()\n",
        "    df_additional = df_additional[df_clean.columns]\n",
        "    df_clean = pd.concat([df_clean, df_additional]).sample(n=2000000, random_state=42)\n",
        "print(f\"Final dataset size: {len(df_clean)} rows\")\n",
        "\n",
        "# Step 3: Load data into PostgreSQL\n",
        "db_url = 'postgresql+psycopg2://username:password@host:port/dbname'  # Replace with ElephantSQL details\n",
        "engine = create_engine(db_url)\n",
        "chunk_size = 100000\n",
        "for i in range(0, len(df_clean), chunk_size):\n",
        "    chunk = df_clean[i:i + chunk_size]\n",
        "    chunk.to_sql('taxi_trips', engine, if_exists='append', index=False)\n",
        "    print(f\"Loaded {i + len(chunk)} rows\")\n",
        "print(\"Data loading complete!\")\n",
        "\n",
        "# Step 4: Validate with a sample query\n",
        "query = \"\"\"\n",
        "SELECT pickup_hour, AVG(fare_amount) as avg_fare, AVG(trip_duration) as avg_duration\n",
        "FROM taxi_trips\n",
        "GROUP BY pickup_hour\n",
        "ORDER BY pickup_hour;\n",
        "\"\"\"\n",
        "result = pd.read_sql(query, engine)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ZaAMdD4dTlg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}